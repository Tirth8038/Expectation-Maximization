# Expectation-Maximization
Expectation Maximization is an iterative algorithm for calculating the maximum likelihood estimates(MLE) or Maximum a posteriori probability (MAP) of parameters. It helps to estimate the missing values in the dataset given the general form of probability distribution associated with these latent variables and then using that data to update the values of the parameters in the Maximization step.  In this task, we do not know whether Coin A or Coin B is flipped for each set of 30 flips.Hence, In this scenario, the coin is not observed, and could be considered a hidden or latent variable.   Initialization Step:  We can initialize random biases for the selection of the Coin which can give the estimate of which coin was chosen in each trial.  Expectation Step:  Given the estimation of the coin selected, we can determine what is the probability of getting heads or tails in the outcome using the concept of Conditional Probability(Bayes Theorem). One approach could be to see which coin bias better matches the flips and assign all flips to that coin. For example: For a flip, if we see 11110011001110001111 and our current assumed biases for Coin A and Coin B are 0.4 and 0.7 respectively, we just assume that it is coin B with 13 H and 7 T. But the problem arises when the cases are not obvious like if the Coin has equal chances of getting H or T.  Hence, we estimate the probability that each coin is the true coin given the flips we see in the trial, and use that to assign H and T counts to each coin. As a final result, it gives the probability of H and T for Coin A and Coin B respectively for each flips.  Maximization Step:  Given the values for the latent variables we computed in the Expectation step, we estimate new values for thetas for both Coins that maximize a variant of the likelihood function. The Theta values will be the probability of H from Coin A and Coin B out of the total probability of both H and T from Coin A and Coin B.  Convergence Step:  After getting the values of Thetas for both Coins, we need to optimize them using a number of iterations till they converge to the global minima. Here, we check whether the Thetas are converging or not, if yes, then stop, otherwise repeat the “Expectation” step and “Maximization” step until the convergence occurs.  For this, I am using a dynamic number of iterations by comparing the values of Theta with Theta values of previous 2 iterations. If it remains constant, I will break the loop and store the last Theta values as Optimal Values.
